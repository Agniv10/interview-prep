-----Deloitte interview QS-------------

what is the loss fn in svm?
what is hinge loss?
binary and categorical cross entropy
bleu score and rogue score, bertscore
metrics of regression
Bert architecture/ how to Bert works internally
how to check if two distribution is similar 
how to keep track of incoming data quality in production

Qs: Longest substring in palindrome:
text='madam'
longest_subs=''
for i in range(0,len(text)):
  for j in range(i+1,len(text)+1):
    substring = text[i:j]
    if substring == substring[::-1] and len(substring)>len(longest_subs):
      longest_subs = substring

Qs: Suppose we are given an array of n integers which represent the value of some stock over  time. Assuming you are allowed to buy the stock exactly once and sell the stock once, what is  the maximum profit you can make? Can you write an algorithm that takes in an array of  values and returns the maximum profit? 
For example, if you are given the following array: 
[2, 7, 1, 8, 2, 8, 14, 25, 14, 0, 4, 5]

1 def version1_SellProfit(n): 
2 max_profit = 0; 
3 for i in range(0, len(n)): 
4 for j in range (i + 1, len(n)): 
5 max_profit = max(max_profit, n[j] - n[i]) 
6 
7 return max_profit

Qs: Given a paragraph, find the top_n words excluding the stopwords.
with open(file_name, 'r') as file:
        # read the file and split it into words
        words = file.read().lower().split()
        
        # remove stopwords from the list of words
        words = [word for word in words if word not in stop_words]
        
        # count the frequency of each word
        word_counts = Counter(words)
        
        # return the top n most frequent words
        return word_counts.most_common(n)


Qs: i have a dataframe df, one col is quantity, i want to filter data where quantity is greater than avg of quantity

# Create example DataFrame
df = pd.DataFrame({'quantity': [3, 5, 8, 2, 4, 9, 1, 6, 7]})

# Calculate the average quantity
avg_quantity = df['quantity'].mean()

# Filter the DataFrame where quantity is greater than the average quantity
filtered_df = df[df['quantity'] > avg_quantity]

# Print the filtered DataFrame
print(filtered_df)

COMPANY: ERM 
Difference between encoders and decoders in transformers
Can we generate texts using Bert models/ architecture?
What is self-attention?
What is Query, value terms wrt self-attention in bert?


Qs: What is masked language modeling?
Ans: Masked language modeling predicts a masked token in a sequence, and the model can attend to tokens bidirectionally. This means the model has full access to the tokens on the left and right. Masked language modeling is great for tasks that require a good contextual understanding of an entire sequence. BERT is an example of a masked language model.


When do we use Random forest?
Ans: Random Forest is a popular machine learning algorithm, especially when dealing with:

1. **High-dimensional data**: With a large number of features, Random Forest can handle the curse of dimensionality better than other methods, such as decision trees or linear models.
2. **Complex relationships**: Random Forest can capture non-linear relationships between variables, making it suitable for problems where relationships are complex and non-linear.
3. **Noise and outliers**: Random Forest is robust to noise and outliers in the data, which makes it a good choice when dealing with datasets that are prone to noise or outliers.
4. **Many classes**: Random Forest is particularly effective when dealing with multiclass classification problems, where the number of classes is large.

Random Forest can handle both discrete and ordinal data, and it is particularly effective when dealing with:
1. **Discrete data**: Random Forest can handle discrete data, such as categorical features, without requiring any additional processing.
2. **Ordinal data**: Random Forest can also handle ordinal data, where the order of the categories is important, by using a technique called "ordinal encoding".
What is overfitting
You have a list of random numbers, find 2nd largest number without sorting

Question: What is positional encoding, why is it needed?
Solution: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/

Question: What is self attention?
Solution: https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/


Question: what is warmup in deep learning?
Answer: During the warmup phase, the learning rate is set to a very low value, typically a small fraction of the final learning rate. This allows the model to learn the simplest patterns in the data, such as the overall shape of the data distribution, without being overwhelmed by the noise and complexity of the data.

As the model becomes more confident in its understanding of the data, the learning rate is gradually increased, typically using a schedule such as a linear increase or a cosine annealing schedule. This allows the model to learn more complex patterns and relationships in the data, and to refine its understanding of the data distribution.
Here are some benefits of warmup in deep learning:

1. **Improved convergence**: Warmup helps the model to converge faster and more accurately by allowing it to learn the simplest patterns in the data before increasing the learning rate.
2. **Reduced overfitting**: By gradually increasing the learning rate, warmup helps to prevent overfitting by allowing the model to learn more complex patterns and relationships in the data.
3. **Faster training**: Warmup can help to speed up the training process by allowing the model to learn the simplest patterns in the data quickly and accurately.

Some common warmup techniques used in deep learning include:

1. **Linear warmup**: The learning rate is increased linearly from a starting value to a final value over a specified number of iterations.
2. **Cosine annealing warmup**: The learning rate is increased using a cosine annealing schedule, which helps to avoid overshooting and oscillations.
3. **Warmup with a fixed number of iterations**: The model is trained for a fixed number of iterations at a low learning rate before increasing the learning rate.


WHAT IS BACKPROPAGATION? url: https://rentry.org/llm-training
In a model, we have many layers that work together to process our data. Think of these layers as interconnected building blocks. When we pass our data through the model, it goes through each of these layers, step by step, in a forward direction. As it travels through the layers, the model makes predictions for the data.

After the data has gone through all the layers and the model has made predictions, we need to measure how accurate or "right" the model's predictions are. We do this by calculating a value called the "loss." The loss tells us how much the model deviates from the correct answers for each data sample.

Now comes the interesting part. We want our model to learn from its mistakes and improve its predictions. To do this, we need to figure out how the loss value changes when we make small adjustments to the model's internal parameters, like the weights and biases.

This is where the concept of gradients comes in. Gradients help us understand how the loss value changes with respect to each model parameter. 

Once we have the gradients, we can use them to update the model's parameters and make them better. We choose an optimizer, which is like a special algorithm responsible for guiding these parameter updates. The optimizer takes into account the gradients, as well as other factors like the learning rate (how big the updates should be) and momentums (which help with the speed and stability of learning).


WHAT IS BATCH AND EPOCHS IN DL?
Assume you have a dataset with 200 samples (rows or sequences of data) and you choose a batch size of 5 and 1,000 epochs. This means that the dataset will be divided into 40 batches, each with five samples. The model weights will be updated after each batch of five samples. This will also mean that one epoch will involve 40 batches or 40 updates to the model.
With 1,000 epochs, the model will be exposed to the entire dataset 1,000 times. That is a total of 40,000 batches during the entire training process.


WHAT IS GRADIENT ACCUMULATION?

Higher batch sizes result in higher memory consumption. Gradient accumulation aims to fix this.
Gradient accumulation is a mechanism to split the batch of samples - used for training your model - into several mini-batches of samples that will be run sequentially.
To understand gradient accumulation, let's think about splitting a batch of samples into smaller groups called mini-batches. In each step, we process one of these mini-batches without updating the model's variables. This means that the model uses the same set of variables for all the mini-batches.
By not updating the variables during these steps, we ensure that the gradients and updates calculated for each mini-batch are the same as if we were using the original full batch. In other words, we guarantee that the sum of the gradients obtained from all the mini-batches is equal to the gradients obtained from the full batch.

What two models are compared while calculating R2 in a regression setup?
It is a scale invariant statistic that gives the amount of variance in the target variable explained by the lin reg model.
r2  = (tss-rss)/tss
    = explained variation/total variation
    = 1-(tss/rss)
rss= residual sum of squares= SUM(y-yi)^2
rss gives the total variation of target variable that is not explained by model.
tss = total sum of squares
tss = SUM(y-Ymean)^2 
tss gives the total variation of target variable

    
When calculating the R² (coefficient of determination) in a regression setup, two models are compared:

1. **The Regression Model (Fitted Model):** This is the model you've trained on your data, which includes the predictors (independent variables) to predict the response (dependent variable). It represents the model you’re evaluating.

2. **The Baseline Model (Mean Model):** This model predicts the mean of the dependent variable for all observations, regardless of the input features. It's a simple model that doesn't use any predictors and serves as a benchmark.

R² essentially measures how well your regression model performs compared to this baseline model. If your model is no better than simply predicting the mean of the dependent variable, R² will be close to 0. If your model perfectly predicts the dependent variable, R² will be 1.


HOW DO YOU EVALUATE CLUSTERING ALGORITHMS?
1. **Internal Indexes**: These methods evaluate the clustering quality based on the data itself, without considering any external information. Some popular internal indexes are:
	* Silhouette Coefficient: measures the distance between each point and the nearest cluster center, and compares it to the distance to the next nearest cluster center.
	* Calinski-Harabasz Index: measures the ratio of between-cluster variance to within-cluster variance.
  * Dunn Index


GINI IMPURITY:
**Gini Coefficient in Decision Trees:**

* Measures the impurity of a node in a decision tree
* Used to select the best feature to split at a node
* Range: [0, 0.5]
* 0: Node is pure (all instances have the same class label)
* 0.5: Node is maximally impure (instances have a uniform distribution of class labels)

In decision trees, the Gini coefficient is used to evaluate the impurity of a node and determine the best feature to split on. A lower Gini coefficient indicates a more pure node, while a higher Gini coefficient indicates a more impure node.


ACTIVATION FUNCTIONS

Sigmoid Activation Function
Definition:

The sigmoid function transforms input values into a range between 0 and 1. Its mathematical expression is:
sigmoid

sigmoid(x)= 1/(1+e^-x)
 
When and Where It’s Needed:

Use Case: Primarily used in binary classification tasks.
Location: Often utilized in the output layer of a neural network designed for binary outcomes.
Why It’s Needed:

Probability Interpretation: Outputs can be interpreted as probabilities, making it suitable for binary classification.
Smooth Gradient: The sigmoid function is smooth and differentiable, which aids in the optimization process during training.
Limitation: It suffers from the vanishing gradient problem for extreme input values, making it less effective in deeper networks.
ReLU Activation Function
Definition:

ReLU (Rectified Linear Unit) outputs the input directly if it is positive; otherwise, it outputs zero. Its formula is:

ReLU(x)=max(0,x)
When and Where It’s Needed:

Use Case: Commonly used in hidden layers of deep neural networks.
Location: Widely adopted in convolutional neural networks (CNNs) and feedforward networks.
Why It’s Needed:

Efficiency: Computationally efficient and helps in speeding up training.
Non-Linearity: Introduces non-linearity into the model, allowing it to learn complex patterns.
Avoids Vanishing Gradients: Helps mitigate the vanishing gradient problem, promoting better learning in deeper networks.
Limitation: Can lead to "dying ReLUs," where neurons become inactive and always output zero.
Leaky ReLU Activation Function
Definition:

Leaky ReLU is a variation of ReLU that allows a small, non-zero gradient when the input is negative. Its formula is:

Leaky ReLU(x)={ 
x     if x>0
αx    if x≤0
​
where α is a small constant (typically around 0.01).

When and Where It’s Needed:

Use Case: Useful in situations where the standard ReLU might fail, particularly in deeper networks.
Location: Often implemented in hidden layers of deep learning models.
Why It’s Needed:

Prevents Dying ReLUs: Allows a small gradient for negative inputs, preventing neurons from becoming inactive.
Retains Information: Helps the network learn from all inputs, including those that are negative, which can enhance model performance.
Balance: Offers a balance between the advantages of ReLU and the need to keep neurons active during training.

Softmax Activation Function
Definition:
The softmax function is an activation function that converts a vector of raw scores (logits) into probabilities. It ensures that the output values are between 0 and 1 and that they sum up to 1. The formula for the softmax function for a vector 

For relu, all negative outputs gets truncated to 0, which causes instability in network. So we use softmax, that takes a negative value and transforms it to small positve value by getting the exponential of the negative number. then it normalizes all the outputs to have probability distribution for all classes.

When and Where It’s Needed:

Use Case: Commonly used in multi-class classification problems.
Location: Typically employed in the output layer of neural networks when you want to classify inputs into multiple classes.
Why It’s Needed:

Probability Distribution: Softmax transforms the raw scores into a probability distribution, allowing for a clear interpretation of the model's outputs as probabilities of each class.
Multi-Class Classification: It allows the model to handle multiple classes effectively by providing a way to select the class with the highest probability.
Differentiability: The softmax function is differentiable, which is essential for backpropagation during training.
Normalization: It normalizes the output, preventing issues that can arise from using raw logits directly, such as numerical instability.


Summary
Sigmoid: Ideal for binary outputs; smooth and interpretable as probabilities, but can face vanishing gradient issues.
ReLU: Efficient for hidden layers, introduces non-linearity, and helps avoid vanishing gradients but can lead to dead neurons.
Leaky ReLU: Addresses the dying ReLU problem by allowing a small gradient for negative inputs, enhancing learning in deeper networks.



ERICSSON INTERVIEW
1. what are the blocks of an transformer?
2. what is the first block of it? Ans: embedding layer/block
3. How embeddings are generated in the 1st layer?
4. how positional encodings work? formula for sinusoidal and cosine in positional encodings?
5. when sine and when cosine functions are used in positional encoding ?
6. Attention mechanism: how it works with mathematics
7. Difference between flan t5 and t5?
8. Difference between rank and rerank
9. what is instance method/ class method/ static method?
10. what is f-beta in ML?
11. How to reduce overfitting in Tree based models?
12. 	Table A column A : 1,2,3,4 
	Table B column A : 1,2,3,4,4,3,2,1
what is the result of inner join
13. s='abcd' leftshift=2 output='cdab'
Solution: def myfunc(st, n):
	    extracted_chars=st[:n]
	    remaining_chrs=st[n:]
	    res = remaining_chrs+extracted_chars
	    return res
myfunc("abcd",3)
